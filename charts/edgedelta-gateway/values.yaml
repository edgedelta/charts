# Default values for edgedelta.

# Use either apiKey or secretApiKey.value but NOT both to provide API Key to Agent
apiKey: ""

secretApiKey:
  name: ""
  key: "ed-api-key"
  value: ""
  enable: false # set true when creating template from charts

# Predefined environment variables
# See: https://docs.edgedelta.com/helm-values/

storePort: ""
profilerPort: ""
promPort: ""
httpProxy: ""
httpsProxy: ""
noProxy: ""
edBackendDisabled: ""
edTagOverride: ""
edClusterName: ""
edSuppressionMode: ""
edSkipConfDownload: ""
edWorkflows: ""
edWorkflowPrefixes: ""
edDisableLeaderElection: ""
edTraceFiles: ""
edAggregatorTraceFiles: ""
edSkipTlsVerify: ""
edConfigContent: ""
goMemLimit: ""

edEnableControllerDiscovery: true

# skipping adding common labels for helm charts into direct helm to k8s yaml file creation.
skipCommonLabels: false

# custom tags are comma separated key:val pairs. these are going to be attached to all outgoing data from agent to configured destination(s).
edCustomTags: ""


# Volume props will be used for mounting certain volume(s) to the given path(s) for the agent
# volumeProps:
#   volumeMounts:
#     - mountPath: /var/lib/kubelet
#       name: varlibkubelet
#       readOnly: true
#   volumes:
#     - hostPath:
#         path: /var/lib/kubelet
#         type: ""
#       name: varlibkubelet

# Custom labels:

# labels:
#   sample-label-key: sample-label-value

# Custom environment variables:

# envs:
#  - name: sample-var
#    value: sample-value

# Custom secret environment variables:
# Use only lowercase alphabetic and - characters in secretKeyRef name and key fields, otherwise helm deploys the release but throws an obscure error about DNS-1123 subdomain format

# secrets:
#  - name: sampleSecretVar
#    secretKeyRef:
#      name: sample-k8-secret-name
#      key: sample-k8s-secret-subkey

# Create secrets with below command:
# kubectl create secret generic sample-k8-secret-name --namespace=edgedelta --from-literal=sample-k8s-secret-subkey="SECRET_VALUE"

# Configuration of deployment type for processor agents
deployment:
  # deployment.replicas -- Number of pods of the deployment. Mutually exclusive with autoscaling
  replicas: 2
  # Configuration of HorizontalPodAutoscaler for processor agents
  autoscaling:
    # deployment.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for processor agents
    enabled: false
    # deployment.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # deployment.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 1
    # deployment.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # deployment.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # deployment.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # deployment.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # deployment.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 60  # Slower scale-up
        policies:
        - type: Percent
          value: 50 # Scale up by 50% max
          periodSeconds: 60
      scaleDown:
        stabilizationWindowSeconds: 300
  # deployment.topologySpreadConstraints -- Topology spread constraints for processor agents while in Deployment mode
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: processor

# Resource constraints
annotations: {}
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 200m
    memory: 256Mi

tolerations: {}
nodeSelector: {}
priorityClassName: ""

updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

coordinatorProps:
  enabled: true
  endpoint: ""

# ports will define which extra ports should be exposed in order to send data to agents
ports: []
# example configuration can be found below:
# ports:
# - name: port-1
#   protocol: TCP
#   port: 80
# - name: port-2
#   protocol: UDP
#   port: 5000

# Helm overrides
repository: gcr.io/edgedelta
image:
  name: agent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
  # Overrides full image (with url and version/tag) if not empty
  # example: gcr.io/edgedelta/agent@sha256:0180ff40f52528d7462200c40fae8aff3bdb83289ac19ce823a3fcb1c40c9ad9
  fullPath: ""
  pullPolicy: IfNotPresent
  pullSecrets: []

# nameOverride -- Override the name of resources.
nameOverride: ""
# fullnameOverride -- Override the full name of resources.
fullnameOverride: ""

# networkPolicy -- Manage NetworkPolicy
networkPolicy:
  # networkPolicy.enabled -- If true, create NetworkPolicy for DaemonSet
  enabled: false
  # networkPolicy.type -- Type of the network policy to use.
  # Can be:
  # * cilium     for cilium.io/v2/CiliumNetworkPolicy
  type: cilium
  cilium:
    # networkPolicy.cilium.dnsSelector -- Cilium selector of the DNSâ€¯server entity
    # @default -- kube-dns in namespace kube-system
    dnsSelector:
      toEndpoints:
        - matchLabels:
            "k8s:io.kubernetes.pod.namespace": kube-system
            "k8s:k8s-app": kube-dns

    # networkPolicy.cilium.customEndpoints -- Custom endpoint addresses for egress traffic
    #
    # customEndpoints:
    #   toFQDNs:
    #     - matchName: "api.edgedelta.com"
    #     - matchPattern: "*.s3.us-east-1.amazonaws.com"
    #   toCIDR:
    #     - 192.168.1.0/24
    #   toServices:
    #     - k8sService:
    #         namespace: default
    #         serviceName: customservice
    #   toPorts:
    #     - ports:
    #         - port: "443"
    #           protocol: TCP

# serviceMonitor -- it will be used enable prometheus to scrape metrics from processor agents
serviceMonitor:
  # serviceMonitor.enabled -- If true, create ServiceMonitor for processor agents
  enabled: false
  #   namespace: monitoring
  #   scrapeTimeout: 10s
  #   jobLabel: edgedelta
  #   selector:
  #     release: prom-kube-stack

instructionURL: "https://app.edgedelta.com"

compactorProps:
  enabled: true
  port: 9199
  # compactorProps.replicas -- Number of compactor agents to be created statically, mutually exclusive with autoscaling
  replicas: 1
  serviceDNSSuffix: svc.cluster.local
  traceFiles: ""
  tolerations: {}
  nodeSelector: {}
  priorityClassName: ""
  updateStrategy:
    type: RollingUpdate
  goMemLimit: ""
  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 300Mi
  # envs:
  # - name: XXX
  #   value: YYY
  # Configuration of HorizontalPodAutoscaler for compactor agents
  autoscaling:
    # compactorProps.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for compactor agents
    enabled: false
    # compactorProps.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # compactorProps.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 1
    # compactorProps.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # compactorProps.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # compactorProps.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # compactorProps.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # compactorProps.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
  # compactorProps.topologySpreadConstraints -- Topology spread constraints for compactor agents
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: compactor

rollUpProps:
  enabled: true
  port: 9200
  replicas: 2
  serviceDNSSuffix: svc.cluster.local
  tolerations: {}
  nodeSelector: {}
  priorityClassName: ""
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  goMemLimit: ""
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  # envs:
  # - name: XXX
  #   value: YYY
  # Configuration of HorizontalPodAutoscaler for rollup agents
  autoscaling:
    # rollUpProps.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for rollup agents
    enabled: false
    # rollUpProps.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # rollUpProps.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 2
    # rollUpProps.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # rollUpProps.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # rollUpProps.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # rollUpProps.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # rollUpProps.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
  # rollUpProps.topologySpreadConstraints -- Topology spread constraints for rollup agents
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: rollup

serviceAccount:
  # serviceAccount.annotations -- Annotations for the service account
  labels: {}
  # serviceAccount.labels -- Labels for the service account
  annotations: {}

podSecurity:
  securityContextConstraints:
    # podSecurity.securityContextConstraints.create -- If true, create a SecurityContextConstraints resource for pods
    create: false

  # podSecurity.capabilities -- Allowed capabilities
  capabilities:
    - SYS_ADMIN
    - SYS_RESOURCE
    - SYS_PTRACE
    - NET_ADMIN
    - NET_BROADCAST
    - NET_RAW
    - IPC_LOCK
    - CHOWN
    - AUDIT_CONTROL
    - AUDIT_READ
    - DAC_READ_SEARCH

  # podSecurity.volumes -- Allowed volumes types
  volumes:
    - configMap
    - downwardAPI
    - emptyDir
    - hostPath
    - secret

  # podSecurity.seLinuxContext -- Provide seLinuxContext configuration for SCC
  # @default -- Must run as spc_t (For reference, please refer here: https://access.redhat.com/solutions/7025337)
  seLinuxContext:
    type: MustRunAs
    seLinuxOptions:
      user: system_u
      role: system_r
      type: spc_t
      level: s0

  # podSecurity.seccompProfiles -- Allowed seccomp profiles
  seccompProfiles:
    - "runtime/default"

  apparmor:
    # podSecurity.apparmor.enabled -- If true, it will enable apparmor for the pods
    enabled: false
    # podSecurity.apparmor.profile -- If apparmor enabled, it will be the profile for apparmor enforcement for the pods
    profile: "unconfined"

  # podSecurity.privileged -- If true, allow to run privileged containers. If eBPF tracer is enabled, this will be automatically true
  privileged: false