# Default values for edgedelta.

# Use either apiKey or secretApiKey.value but NOT both to provide API Key to Agent
apiKey: ""

secretApiKey:
  name: "ed-api-key"
  key: "ed-api-key"
  value: ""
  enable: false # set true when creating template from charts

# Predefined environment variables
# See: https://docs.edgedelta.com/installation/environment-variables

storePort: ""
profilerPort: ""
promPort: ""
httpProxy: ""
httpsProxy: ""
noProxy: ""
edBackendDisabled: ""
edTagOverride: ""
edSuppressionMode: ""
edSkipConfDownload: ""
edWorkflows: ""
edWorkflowPrefixes: ""
edDisableLeaderElection: ""
edTraceFiles: ""
edAggregatorTraceFiles: ""
edSkipTlsVerify: ""
edConfigContent: ""
goMemLimit: "1800MiB"

edEnableControllerDiscovery: true

# skipping adding common labels for helm charts into direct helm to k8s yaml file creation.
skipCommonLabels: false

# custom tags are comma separated key:val pairs. these are going to be attached to all outgoing data from agent to configured destination(s).
edCustomTags: ""

# Persisting Cursor props constraints
persistingCursorProps:
  enabled: true
  hostMountPath: "/var/lib/edgedelta"
  containerMountPath: "/var/lib/edgedelta"

# Docker container properties
dockerContainerProps:
  hostPath: "/var/lib/docker/containers"

# Volume props will be used for mounting certain volume(s) to the given path(s) for the agent
# volumeProps:
#   volumeMounts:
#     - mountPath: /var/lib/kubelet
#       name: varlibkubelet
#       readOnly: true
#   volumes:
#     - hostPath:
#         path: /var/lib/kubelet
#         type: ""
#       name: varlibkubelet

# Agent properties
agentProps:
  image:
    pullPolicy: IfNotPresent

# Aggregator agent mode properties
# Agents are deployed as daemonset. an extra aggregator agent is deployed to collect/aggregate metrics
aggregatorProps:
  enabled: false
  port: 9191
  # serviceDNSSuffix is "svc.cluster.local" by default. It's used to construct endpoint for aggregator agent service so that processor agents can communicate with aggregator.
  # Final endpoint looks like this: http://ed-aggregator-svc.my-namespace.svc.cluster.local:9191
  serviceDNSSuffix: "svc.cluster.local"
  enabledDataTypes:
    metric: true
    cluster_pattern_and_sample: false
    topk: false

  # aggregator can leverage persistent volume to persist internal source related information to recover quickly after restarts/upgrades.
  # aggregator doesn't necessarily need a persistent volume. It will reconcile overall state  after a restart from other agents by returning them an error and forcing them to share source information
  usePersistentVolume: false
  # storageClassName is used if usePersistentVolume=true
  # If your k8s cluster has specific storage classes you can set storageClassName accordingly.
  storageClassName: ""

  goMemLimit: "1800MiB"
  resources:
    limits:
      cpu: 2000m
      memory: 2048Mi
    requests:
      cpu: 200m
      memory: 256Mi

# HttpRecorder is a frontend layer which can consume logs with both http and tcp protocols
# It is deployed as a sidecar for each ED agent. It dumps the incoming logs to filesystem (persisted via PVC) and agents grab from there.
httpRecorderProps:
  enabled: false
  image:
    fullPath: gcr.io/edgedelta/httprecorder:latest
    pullPolicy: IfNotPresent
  port: 8080

# Custom labels:

# labels:
#   sample-label-key: sample-label-value

# Custom environment variables:

# envs:
#  - name: sample-var
#    value: sample-value

# Custom secret environment variables:
# Use only lowercase alphabetic and - characters in secretKeyRef name and key fields, otherwise helm deploys the release but throws an obscure error about DNS-1123 subdomain format

# secrets:
#  - name: sampleSecretVar
#    secretKeyRef:
#      name: sample-k8-secret-name
#      key: sample-k8s-secret-subkey

# Create secrets with below command:
# kubectl create secret generic sample-k8-secret-name --namespace=edgedelta --from-literal=sample-k8s-secret-subkey="SECRET_VALUE"

# Configuration of deployment type for processor agents
deployment:
  # deployment.kind -- Can be either DaemonSet or Deployment
  kind: DaemonSet
  # deployment.replicas -- Number of pods of the deployment (only applies when kind == Deployment). Mutually exclusive with autoscaling
  replicas: 1
  # Configuration of HorizontalPodAutoscaler for processor agents in Deployment mode
  autoscaling:
    # deployment.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for processor agents
    enabled: false
    # deployment.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # deployment.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 1
    # deployment.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # deployment.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # deployment.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # deployment.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # deployment.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
  # deployment.topologySpreadConstraints -- Topology spread constraints for processor agents while in Deployment mode
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: processor

# Resource constraints
annotations: {}
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 200m
    memory: 256Mi

tolerations: {}
nodeSelector: {}
priorityClassName: ""

updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# ports will define which extra ports should be exposed in order to send data to agents
ports: []
# example configuration can be found below:
# ports:
# - name: port-1
#   protocol: TCP
#   port: 80
#   exposeInHost: true # It will expose port in host, only valid for DaemonSet deployment kind
# - name: port-2
#   protocol: UDP
#   port: 5000
#   exposeInHost: false
pushService:
  type: ClusterIP
  annotations: {}
  loadBalancerIP: ""
  clusterIP: ""
  sessionAffinity: ""
  sessionAffinityTimeout: 10800

# Helm overrides

image:
  name: gcr.io/edgedelta/agent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
  # Overrides full image (with url and version/tag) if not empty
  # example: gcr.io/edgedelta/agent@sha256:0180ff40f52528d7462200c40fae8aff3bdb83289ac19ce823a3fcb1c40c9ad9
  fullPath: ""
imagePullSecrets: []

# nameOverride -- Override the name of resources.
nameOverride: ""
# fullnameOverride -- Override the full name of resources.
fullnameOverride: ""

# networkPolicy -- Manage NetworkPolicy
networkPolicy:
  # networkPolicy.enabled -- If true, create NetworkPolicy for DaemonSet
  enabled: false
  # networkPolicy.type -- Type of the network policy to use.
  # Can be:
  # * cilium     for cilium.io/v2/CiliumNetworkPolicy
  type: cilium
  cilium:
    # networkPolicy.cilium.dnsSelector -- Cilium selector of the DNSâ€¯server entity
    # @default -- kube-dns in namespace kube-system
    dnsSelector:
      toEndpoints:
        - matchLabels:
            "k8s:io.kubernetes.pod.namespace": kube-system
            "k8s:k8s-app": kube-dns

    # networkPolicy.cilium.customEndpoints -- Custom endpoint addresses for egress traffic
    #
    # customEndpoints:
    #   toFQDNs:
    #     - matchName: "api.edgedelta.com"
    #     - matchPattern: "*.s3.us-east-1.amazonaws.com"
    #   toCIDR:
    #     - 192.168.1.0/24
    #   toServices:
    #     - k8sService:
    #         namespace: default
    #         serviceName: customservice
    #   toPorts:
    #     - ports:
    #         - port: "443"
    #           protocol: TCP

# serviceMonitor -- it will be used enable prometheus to scrape metrics from processor agents
serviceMonitor:
  # serviceMonitor.enabled -- If true, create ServiceMonitor for processor agents
  enabled: false
  #   namespace: monitoring
  #   scrapeTimeout: 10s
  #   jobLabel: edgedelta
  #   selector:
  #     release: prom-kube-stack

instructionURL: "https://app.edgedelta.com"

agentUpdater:
  enabled: false
  image: gcr.io/edgedelta/agent-updater:latest
  baseURL: https://api.edgedelta.com/v1
  latestTagEndpoint: /versioning/latest
  metadataEndpoint: /agent_updater/metadata
  logUploader:
    enabled: true
    presignedUploadURLEndpoint: /agent_updater/self_logs_upload_link

tracerProps:
  enabled: true
  port: 9595

compactorProps:
  enabled: true
  port: 9199
  usePVC: false
  storageClass: ""
  diskSize: 30Gi
  # compactorProps.replicas -- Number of compactor agents to be created statically, mutually exclusive with autoscaling
  replicas: 1
  serviceDNSSuffix: svc.cluster.local
  traceFiles: ""
  podManagementPolicy: "OrderedReady"
  tolerations: {}
  nodeSelector: {}
  priorityClassName: ""
  image:
    pullPolicy: IfNotPresent
  updateStrategy:
    type: RollingUpdate
  goMemLimit: "1800MiB"
  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 200m
      memory: 300Mi
  # Configuration of HorizontalPodAutoscaler for compactor agents
  autoscaling:
    # compactorProps.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for compactor agents
    enabled: false
    # compactorProps.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # compactorProps.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 1
    # compactorProps.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # compactorProps.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # compactorProps.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # compactorProps.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # compactorProps.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
  # compactorProps.topologySpreadConstraints -- Topology spread constraints for compactor agents
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: compactor

rollUpProps:
  enabled: true
  port: 9200
  replicas: 2
  serviceDNSSuffix: svc.cluster.local
  podManagementPolicy: "OrderedReady"
  tolerations: {}
  nodeSelector: {}
  priorityClassName: ""
  image:
    pullPolicy: IfNotPresent
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  goMemLimit: "900MiB"
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  # Configuration of HorizontalPodAutoscaler for rollup agents
  autoscaling:
    # rollUpProps.autoscaling.enabled -- Create a [HorizontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for rollup agents
    enabled: false
    # rollUpProps.autoscaling.external -- Set to `true` if using an external autoscaler like [KEDA](https://keda.sh/)
    external: false
    # rollUpProps.autoscaling.minReplicas -- Minimum replica count for rollup agents
    minReplicas: 2
    # rollUpProps.autoscaling.maxReplicas -- Maximum replica count for rollup agents
    maxReplicas: 10
    # rollUpProps.autoscaling.targetForCPUUtilizationPercentage -- Targeted CPU utilization for rollup agents in order to HPA to kick in
    targetForCPUUtilizationPercentage: 85
    # rollUpProps.autoscaling.targetForMemoryUtilizationPercentage -- Targeted Memory utilization for rollup agents in order to HPA to kick in
    targetForMemoryUtilizationPercentage:
    # rollUpProps.autoscaling.customMetric -- For any custom metrics for targeting, one can use this section
    customMetric:
      {}
      #  - type: Pods
      #    pods:
      #      metric:
      #        name: packets-per-second
      #      target:
      #        type: AverageValue
      #        averageValue: 1k
    # rollUpProps.autoscaling.behavior -- Configure separate scale-up and scale-down behaviors
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
  # rollUpProps.topologySpreadConstraints -- Topology spread constraints for rollup agents
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  #   labelSelector:
  #     matchLabels:
  #       edgedelta/agent-type: rollup

# Configuration for Ingress to add in front of ports defined above,
# requires Kubernetes >= 1.19
ingress:
  # ingress.enabled -- If set to true, we will create and use an Ingress resource
  enabled: false
  # ingress.class -- Specify the [ingressClassName](https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class)
  class: ""
  # ingress.annotations -- Set annotations for further ingress configuration
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  # ingress.hosts -- Configure the hosts and paths
  hosts: []
  #  - host: host.example.local
  #    paths:
  #      - path: /
  #        pathType: ImplementationSpecific
  #        # Specify the port name or number from ports section
  #        port:
  #          name: ""
  #          number: ""
  # ingress.tls -- TLS configurations
  tls: []
  #  - hosts:
  #      - host.example.local
  #    secretName: example-tls-secret

forceReinstallApplications:
  # forceReinstallApplications.enabled -- When set to true, it will delete some of the K8s applications to be reinstalled again
  enabled: true
